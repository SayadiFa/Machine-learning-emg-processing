# -*- coding: utf-8 -*-
"""elie bitar

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12uNMjkqMjiLgLe7kcrrUJCJfrzFKQ0fl
"""

! curl -O https://data.4tu.nl/file/318e6d0f-122a-4c19-820d-05c5cfb1dd50/e55b01eb-6818-4e8c-82d0-af01055bfcde

! pip install rarfile
import rarfile
rar_path = "e55b01eb-6818-4e8c-82d0-af01055bfcde"
rar = rarfile.RarFile(rar_path)

import scipy.io

path_to_extract = "data"
rar.extractall(path_to_extract)

import pandas as pd
txt_path = "data/HS4/Bad Electrodes.txt"
data = pd.read_csv(txt_path, sep='\t')
print(data)

with open('data/HS4/Measurements HS4.txt', 'r') as file:
    content = file.read()
print(content)

!pip install pysiology

from sklearn.preprocessing import MinMaxScaler
import numpy as np
import scipy.signal as signal
import pysiology.electromyography as emg
import scipy.io
import pandas as pd
import matplotlib.pyplot as plt

datae= scipy.io.loadmat("data/DS1/thumb_flex_raw.mat", mat_dtype=True, squeeze_me=True, struct_as_record=False, chars_as_strings=True, matlab_compatible=False)
data =datae["thumb_flex_ds1"]

import os
import re
folder_path = './data'
pattern = r"HS\d$"
dict_data = {'MNF': [], 'MNP': [],'TTP': [], 'SM1': [],'SM2': [], 'SM': []
  ,'IEMG': [], 'MAV': [], 'MAV1': [],
 'MAV2': [], 'SSI': [], 'VAR': [], 'TM': [],
 'LOG': [], 'RMS': [], 'WL': [], 'AAC': [], 'DASDV': [], 'ZC': [], 'MYOP':[],
 'WAMP': [], 'SSC': [], 'MAVSLPk1':[],"MAVSLPk2":[],"MAVSLPk3":[],"psd":[],"frequencies":[],"label":[]}

# Iterate over folders
for root, dirs, files in os.walk(folder_path):
    for file_name in files:
        file_path = os.path.join(root, file_name)
        if file_path.endswith(".mat"):
          data = scipy.io.loadmat(file_path, mat_dtype=True, squeeze_me=True, struct_as_record=False, chars_as_strings=True, matlab_compatible=False)
          for i in data[list(data.keys())[-1]]:
            emg_data = i[:256]
            scaler = MinMaxScaler(feature_range=(0, 10))
            scaled_emg_data = scaler.fit_transform(emg_data.reshape(-1, 1)).flatten()
            samplerate = 200
            frequencies, psd = signal.welch(scaled_emg_data, fs=samplerate, window='hamming', nperseg=256, detrend='constant', scaling="spectrum")
            threshold = 0.5
            order = 4
            SM1 = np.sum(psd * frequencies) / np.sum(psd)
            SM2= np.sqrt(np.sum(psd * (frequencies - SM1)**2) / np.sum(psd))
            if re.search(pattern, root):
              dict_data["label"].append(0)
            else :
              dict_data["label"].append(1)
            dict_data["SM1"].append(SM1)
            dict_data["SM2"].append(SM2)
            dict_data["psd"].append(psd.mean())
            dict_data["frequencies"].append(frequencies.mean())
            dict_data["AAC"].append( emg.getAAC(scaled_emg_data))
            dict_data["DASDV"].append( emg.getDASDV(scaled_emg_data))
            dict_data["IEMG"].append( emg.getIEMG(scaled_emg_data))
            dict_data["LOG"].append( emg.getLOG(scaled_emg_data))
            dict_data["MAV"].append( emg.getMAV(scaled_emg_data))
            dict_data["MAV1"].append( emg.getMAV1(scaled_emg_data))
            dict_data["MAV2"].append( emg.getMAV2(scaled_emg_data))
            dict_data["MAVSLPk1"].append(np.array(emg.getMAVSLPk(scaled_emg_data,nseg=256)).mean())
            dict_data["MAVSLPk2"].append(np.array(emg.getMAVSLPk(scaled_emg_data,nseg=256)).min())
            dict_data["MAVSLPk3"].append(np.array(emg.getMAVSLPk(scaled_emg_data,nseg=256)).max())
            dict_data["MNF"].append( emg.getMNF(scaled_emg_data,frequencies= frequencies))
            dict_data["MNP"].append( emg.getMNP(scaled_emg_data))
            dict_data["MYOP"].append( emg.getMYOP(scaled_emg_data,threshold=threshold))
            dict_data["RMS"].append( emg.getRMS(scaled_emg_data))
            dict_data["SM"].append( emg.getSM(scaled_emg_data,frequencies= frequencies,order=order))
            dict_data["SSC"].append( emg.getSSC(scaled_emg_data,threshold=threshold))
            dict_data["SSI"].append( emg.getSSI(scaled_emg_data))
            dict_data["TM"].append( emg.getTM(scaled_emg_data,order=order))
            dict_data["TTP"].append( emg.getTTP(scaled_emg_data))
            dict_data["VAR"].append(emg.getVAR(scaled_emg_data))
            dict_data["WAMP"].append(emg.getWAMP(scaled_emg_data,threshold=threshold))
            dict_data["WL"].append(emg.getWL(scaled_emg_data))
            dict_data["ZC"].append(emg.getZC(scaled_emg_data,threshold=threshold))



dataFrame = pd.DataFrame(dict_data)
dataFrame = dataFrame.dropna()

dataFrame.columns

dataFrame.iloc[:30,:14]

# Count the number of unique values in each column
unique_counts = dataFrame.nunique()

# Get the list of column names that have only 1 unique value
columns_to_drop = unique_counts[unique_counts == 1].index.tolist()
print(columns_to_drop)

# Drop the columns with only 1 unique value
dataFrame_dropped = dataFrame.drop(columns=columns_to_drop, axis=1)

dataFrame["label"].unique()
label_counts = dataFrame["label"].value_counts()
print(label_counts)

#  healthy signal
hs = dataFrame[dataFrame.label == 0]
ds = dataFrame[dataFrame.label==1]

def plot_scatter(x,y):
  fig, ax = plt.subplots()
  fig.set_size_inches(13, 7)
  # lables and scatter points
  ax.scatter(ds[x], ds[y], label="dmd", facecolor="blue")
  ax.scatter(hs[x], hs[y], label="control", facecolor="green")
  ax.set_xlabel(x)
  ax.set_ylabel(y)
  ax.grid()
  ax.set_title("")
  ax.legend()

plot_scatter('WL', 'AAC')

#EDA
from collections import Counter
# data preprocessing
from sklearn.preprocessing import StandardScaler
# data splitting
from sklearn.model_selection import train_test_split
# data modeling
from sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neural_network import MLPClassifier
# from mlxtend.classifier import StackingCVClassifier

def Main(dataFrame,columns):
    models = {
        'Logistic Regression':LogisticRegression(max_iter=5000,solver='lbfgs', fit_intercept=True,intercept_scaling=1),
        "Naive Bayes":GaussianNB( var_smoothing=1e-018),
        'Support Vector ':SVC(kernel='rbf', C=2, max_iter=-1),
        "Decision Tree":DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 32),
        "Random Forest": RandomForestClassifier(n_estimators=26, random_state=2,max_depth=5),
        "K-Neighbors":KNeighborsClassifier(n_neighbors=16),
        "Extreme Gradient Boost":XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5),
        "Ada Boost":AdaBoostClassifier(n_estimators=500, learning_rate = 0.75),
        "LDA":LinearDiscriminantAnalysis(),
        "MLP":MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001,solver='sgd',  random_state=21,tol=0.000000001)
    }
    acc_score={}
    colors = ['red','green','blue','Cyan','Lime','yellow','orange',"DeepPink","OrangeRed","DarkMagenta"]
    y, X = dataFrame['label'], dataFrame.loc[:,columns].astype('float64')
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    score = []
    models_names = list(models.keys())
    for i in models_names:
      models[i].fit(X_train, y_train)
      predict = models[i].predict(X_test)
      acc = accuracy_score(y_test, predict)
      score.append(acc*100)
    acc_score["Model"] = models_names
    acc_score["Accuracy"] = score
    frame =  pd.DataFrame(acc_score)
    plt.figure(figsize=(20,10))
    plt.title("Barplot represent accuracy of different models", fontsize=22)
    plt.xlabel("Algorithms")
    plt.ylabel("Accuracy %")
    plt.xticks(fontsize=20)
    plt.yticks(fontsize=20)
    plt.bar(frame['Model'],frame['Accuracy'],color = colors)
    plt.xlabel('Algorithms', fontsize=20)  # Set font size for x-label
    plt.ylabel('Accuracy %', fontsize=20)
    # Rotate x-axis tick labels for better readability
    plt.xticks(rotation=45, ha='right')
    plt.show()
    return [frame,models.copy()]

def filter(dataFrame,name):
    mean = dataFrame[name].mean().astype('float64')
    std = dataFrame[name].std().astype('float64')
    data = dataFrame.loc[:,[name,'label']].astype('float64')
    for x in data.index:
      if data.loc[x, name] > mean+std or data.loc[x, name] < mean-std:
        data.drop(x, inplace = True)
    return data

def MainoneByone(dataFrame):
    models = {
        'Logistic Regression': LogisticRegression(max_iter=5000, solver='lbfgs', fit_intercept=True, intercept_scaling=1),
        "Naive Bayes": GaussianNB(var_smoothing=1e-018),
        'Support Vector ': SVC(kernel='rbf', C=2, max_iter=-1),
        "Decision Tree": DecisionTreeClassifier(criterion='entropy', random_state=0, max_depth=32),
        "Random Forest": RandomForestClassifier(n_estimators=26, random_state=2, max_depth=5),
        "K-Neighbors": KNeighborsClassifier(n_neighbors=16),
        "Extreme Gradient Boost": XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15, gamma=0.6, subsample=0.52, colsample_bytree=0.6, seed=27, reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5),
        "Ada Boost": AdaBoostClassifier(n_estimators=500, learning_rate=0.75),
        "MLP": MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=100, alpha=0.0001, solver='sgd', random_state=21, tol=0.000000001)
    }
    columns = list(dataFrame.columns)
    data = {
        "feature": [],
        'Logistic Regression': [],
        "Naive Bayes": [],
        'Support Vector ': [],
        "Decision Tree": [],
        "Random Forest": [],
        "K-Neighbors": [],
        "Extreme Gradient Boost": [],
        "Ada Boost": [],
        "MLP": [],
    }
    reject = []
    models_names = list(models.keys())
    for j in columns[:-1]:
        copy_model = models.copy()
        # filter
        df = filter(dataFrame, j)
        if len(df.index) > 100:
            y, X = df['label'], df[[j]].astype('float64')
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            for i in models_names:
                if i == "LDA":
                    continue  # Skip LinearDiscriminantAnalysis
                copy_model[i].fit(X_train, y_train)
                predict = copy_model[i].predict(X_test)
                acc = accuracy_score(y_test, predict)
                data[i].append(acc * 100)
            data["feature"].append(j)
        else:
            reject.append(j)
    frame = pd.DataFrame(data)
    return [frame, reject]

[acuuracy, rejected] = MainoneByone(dataFrame)

acuuracy

rejected

def filtermultiColumn(dataFrame,selected_features):
  std={}
  mean={}
  for i in selected_features:
    mean[i] = dataFrame[i].mean().astype('float64')
    std[i] =dataFrame[i].std().astype('float64')
  selected_features.append("label")
  df = dataFrame[selected_features]
  for i in selected_features[:-1]:
    for j in df[i].index:
      if df.loc[j, i] > mean[i]+std[i] or df.loc[j, i] < mean[i]-std[i]:
        df = df.drop(j)
  return df

data = filtermultiColumn(dataFrame,list(dataFrame.columns[:-1]))
Main(data,dataFrame.columns[:-1])

from sklearn.svm import SVR
from sklearn.feature_selection import SequentialFeatureSelector

y, X = dataFrame['label'], dataFrame.drop(columns='label').astype('float64')


knn = KNeighborsClassifier(n_neighbors=10)
sfs = SequentialFeatureSelector(knn, n_features_to_select=10)
sfs.fit(X, y)
selected_features = list(X.columns[sfs.get_support()])
selected_features

data1 = filtermultiColumn(dataFrame,selected_features)
Main(data1,selected_features)